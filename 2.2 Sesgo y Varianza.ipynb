{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Sesgo/Bias y Varianza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente información es una recopilación de un video y la documentación original esta en este enlace\n",
    "[Sesgo y Varianza en Machine Learning](https://aprendeia.com/bias-y-varianza-en-machine-learning/) les invito a visitarlo pues su curso en youtube es uno de los más completos de habla hispana.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes saber que en el mundo de Machine Learning, la precisión lo es todo. Cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando y reajustando los parámetros, pero la realidad es que **no se puede construir un modelo 100% preciso** ya que nunca pueden estar libres de errores de lo contrario estariamos viendo el futuro.\n",
    "\n",
    "Comprender cómo las diferentes fuentes de error generan **bias y varianza** te ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitarás el error de sobreajuste y falta de ajuste.\n",
    "\n",
    "Nota: en ocasiones encontrarás la palabra **bias como sesgo**, así es como se le llama en español a este tipo de error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El error de predicción para cualquier algoritmo de Machine Learning se puede dividir en tres partes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://live.staticflickr.com/65535/48057276003_c7f786a1a2_b.jpg){width=50px}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error irreducible \n",
    "El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado.\n",
    "\n",
    "#### <center> No importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar\n",
    "\n",
    "Sin embargo, los otros dos tipos de errores se pueden reducir porque se derivan de la elección del algoritmo, razón por la cual en esta entrada nos enfocaremos en ambos.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error de bias o sesgo\n",
    "\n",
    "#### <center> Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos\n",
    "Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos.\n",
    "\n",
    "Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como ajuste insuficiente.\n",
    "\n",
    "En general, los algoritmos paramétricos como la regresión lineal, tienen un alto bias que los hace rápidos de aprender y más fácil de entender, pero generalmente menos flexibles. A su vez, tienen un menor rendimiento predictivo en problemas complejos.\n",
    "\n",
    "![](https://live.staticflickr.com/65535/48057227231_5aacf1e5ac_b.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bajo bias: sugiere menos suposiciones sobre la forma de la función objetivo. Los algoritmos de Machine Learning con baja bias incluyen: árboles de decisión, k-vecinos más cercanos y máquinas de vectores de soporte.\n",
    "\n",
    "* Alto bias: sugiere más suposiciones sobre la forma de la función objetivo. Los algoritmos con alto bias se incluyen: regresión lineal, análisis discriminante lineal y regresión logística."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error de varianza \n",
    "#### <center> Los algoritmos con alto bias se incluyen: regresión lineal, análisis discriminante lineal y regresión logística\n",
    "\n",
    "La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro, lo que significa que el algoritmo es bueno para elegir el mapeo subyacente oculto entre las variables de entrada y de salida.\n",
    "\n",
    "Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo.\n",
    "\n",
    "Generalmente, los algoritmos de Machine Learning no paramétrico que tienen mucha flexibilidad tienen una gran variación.\n",
    "\n",
    "![](https://live.staticflickr.com/65535/48057275418_bf2cae6f83_b.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Varianza baja**: sugiere pequeños cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación. Los algoritmos de Machine Learning con baja varianza incluye: regresión lineal, análisis discriminante lineal y regresión logística.\n",
    "\n",
    "* **Alta varianza**: sugiere grandes cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación. Los algoritmos con alta varianza son: árboles de decisión, k-vecinos más cercanos y máquinas de vectores de soporte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La compensación Bias-Varianza o Trade-off\n",
    "El objetivo de cualquier algoritmo supervisado de Machine Learning es lograr un bias bajo y una baja varianza, a su vez, el algoritmo debe lograr un buen rendimiento de predicción\n",
    "![](https://live.staticflickr.com/65535/48057226411_5cdfc33307_b.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bias frente a la varianza se refiere a la precisión frente a la consistencia de los modelos entrenados por su algoritmo. Podemos diagnosticarlos de la siguiente manera\n",
    "\n",
    "* Los algoritmos de baja varianza (alto bias) tienden a ser menos complejos, con una estructura subyacente simple o rígida.\n",
    "\n",
    "Entrenan modelos que son consistentes, pero inexactos en promedio. Estos incluyen algoritmos paramétricos o lineales, como la regresión lineal y el ingenuo Bay\n",
    "\n",
    "![](https://live.staticflickr.com/65535/48057321037_d27258215d_b.jpg)\n",
    "\n",
    "* Los algoritmos de bajo bias (alta varianza) tienden a ser más complejos, con una estructura subyacente flexible.\n",
    "\n",
    "Entrenan modelos que son precisos en promedio pero inconsistentes. Estos incluyen algoritmos no lineales o no paramétricos, como árboles de decisión y k-vecinos más cercanos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> No hay escapatoria a la relación entre el bias y la varianza en Machine Learning, aumentar el bias disminuirá la varianza, aumentar la varianza disminuirá el bias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error total\n",
    "\n",
    "Comprender el bias y la varianza es fundamental para comprender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el error general, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el bias es equivalente a la reducción en la varianza.\n",
    "\n",
    "![](https://live.staticflickr.com/7927/46138669365_b98531b89d_b.jpg)\n",
    "\n",
    "Para construir un buen modelo, necesitamos encontrar un buen equilibrio entre el bias y la varianza de manera que minimice el error total.\n",
    "\n",
    "**Un equilibrio óptimo de bias y varianza nunca sobreequiparía o no sería adecuado para el modelo. Por lo tanto comprender el bias y la varianza es fundamental para comprender el comportamiento de los modelos de predicción.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
